{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as npy\n",
    "import cupy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as scio\n",
    "from time import time\n",
    "from eznf import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 辅助函数的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(i: int):\n",
    "    oh = npy.zeros(10)\n",
    "    if(i == 10):\n",
    "        oh[0] = 1\n",
    "    else:\n",
    "        oh[i] = 1\n",
    "    return oh\n",
    "\n",
    "def accuracy(m, x: np.ndarray, y: np.ndarray, batch_size):\n",
    "    return (m(x) == y.argmax(axis=0)).sum() / batch_size\n",
    "\n",
    "def flatten(x: np.ndarray, batch_size: int):\n",
    "    return x.reshape([batch_size, -1], order='C').T\n",
    "\n",
    "def reflatten(x: np.ndarray, shape: list or tuple):\n",
    "    \"\"\"\n",
    "        shape: batch_size * channels * w * h\n",
    "    \"\"\"\n",
    "    return x.T.reshape(shape, order='C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 激活函数和导数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 线性\n",
    "def linear(x: np.ndarray, w: np.ndarray):\n",
    "    if(w.shape[1] != x.shape[0]):\n",
    "        raise ValueError('could not multiply shape {} x {} with {} x {}'.format(*w.shape, *x.shape))\n",
    "    return w @ x\n",
    "\n",
    "# relu激活函数\n",
    "def relu(x: np.ndarray):\n",
    "    return x * (x > 0)\n",
    "\n",
    "# softmax激活函数\n",
    "def softmax(x: np.ndarray):\n",
    "    phi = x.max(axis=0)\n",
    "    e = np.exp(x - phi)\n",
    "    return e / e.sum(axis=0)\n",
    "\n",
    "# 交叉熵损失\n",
    "def cross_entropy(a: np.ndarray, y: np.ndarray, batch_size: int):\n",
    "    return -(y*np.log(a)).sum() / batch_size\n",
    "\n",
    "# 线性导数\n",
    "def dlinear(x: np.ndarray, w: np.ndarray):\n",
    "    return w\n",
    "\n",
    "# relu导数\n",
    "def drelu(x: np.ndarray):\n",
    "    return x > 0\n",
    "\n",
    "\n",
    "# 交叉熵损失导数\n",
    "def dcross_entropy(a: np.ndarray, y: np.ndarray):\n",
    "    return a - y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积和池化及其导数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 卷积, stride = 1, padding = 0\n",
    "def cov2d(x: np.ndarray, w: np.ndarray):\n",
    "    \"\"\"\n",
    "        x: batch_size * channels * w * h\n",
    "        w: out_channels * channels * w * h\n",
    "    \"\"\"    \n",
    "    batch_size, in_channels, x_size, _ = x.shape\n",
    "    out_channels, _, w_size, _ = w.shape\n",
    "    steps = x_size - w_size + 1\n",
    "\n",
    "    _w = w.reshape([out_channels, -1], order='C')   # 将卷积核展平\n",
    "    _z = np.zeros([in_channels*w_size**2, batch_size*steps**2]) # 为将输入展平做准备\n",
    "\n",
    "    for i in range(w_size):\n",
    "        for j in range(w_size):\n",
    "            _z[i*w_size+j::w_size**2, :] = x[:, :, i:steps+i, j:steps+j].transpose(1,0,2,3).reshape([in_channels,-1], order='C')    # 将输入展平(im2col)\n",
    "\n",
    "    z = (_w @ _z).reshape([out_channels, batch_size, steps, steps], order='C').transpose(1,0,2,3)   # 矩阵相乘后还原(卷积)\n",
    "    return z\n",
    "\n",
    "# 最大池化, stride = p_size, padding = 0\n",
    "def max_pooling(x: np.ndarray, p_size: int):\n",
    "    \"\"\"\n",
    "        x: batch_size * channels * w * h\n",
    "    \"\"\"    \n",
    "    batch_size, c, x_size, _ = x.shape\n",
    "\n",
    "    if(int(x_size/p_size) != x_size/p_size):\n",
    "        raise ValueError('x_size must be divided exactly by p_size')\n",
    "    steps = int(x_size/p_size)\n",
    "    _z = np.zeros([p_size**2, batch_size*c*steps**2])\n",
    "    \n",
    "\n",
    "    for i in range(p_size):\n",
    "        for j in range(p_size):\n",
    "            _z[i*p_size+j, :] = x[:, :, i::p_size, j::p_size].reshape([-1], order='C')    # 将输入展平(im2col)\n",
    "\n",
    "    z = _z.max(axis=0).reshape([batch_size, c, steps, steps])\n",
    "    arg_max = _z.argmax(axis=0)\n",
    "    return z, arg_max\n",
    "\n",
    "# 卷积对w导数, stride = 1, padding = 0\n",
    "def dwcov2d(x: np.ndarray, z: np.ndarray):\n",
    "    \"\"\"\n",
    "        x: batch_size * channels * w * h\n",
    "        z: batch_size * channels * w * h\n",
    "    \"\"\"\n",
    "    batch_size, x_c, x_size, _ = x.shape\n",
    "    _, z_c, z_size, _ = z.shape\n",
    "    steps = x_size - z_size + 1\n",
    "\n",
    "    _z = z.transpose(1,0,2,3).reshape([z_c, -1], order='C')\n",
    "    _x = np.zeros([batch_size*z_size**2, steps**2*x_c])\n",
    "    for i in range(steps):\n",
    "        for j in range(steps):\n",
    "            loc = i*steps+j\n",
    "            _x[:,loc::steps**2] =  x[:, :, i:i+z_size, j:j+z_size].transpose(1,0,2,3).reshape([x_c,-1]).T   # 方法同cov2d, 采用im2col算法\n",
    "\n",
    "    dw = (_z @ _x).reshape(z_c,x_c,steps,steps) / batch_size\n",
    "    return dw\n",
    "\n",
    "# 卷积对x导数, stride = 1, padding = 0\n",
    "def dxcov2d(w: np.ndarray, z: np.ndarray):\n",
    "    \"\"\"\n",
    "        w: out_channels * channels * w * h\n",
    "        z: batch_size * channels * w * h\n",
    "    \"\"\"\n",
    "    _, in_c, w_size, _ = w.shape\n",
    "    batch_size, out_c, z_size, _ = z.shape\n",
    "    x_size = z_size + w_size - 1\n",
    "\n",
    "    # time1 = time()\n",
    "    x = np.zeros([batch_size, in_c, x_size, x_size])\n",
    "    for i in range(z_size):\n",
    "        for j in range(z_size):\n",
    "            x[:, :, i:i+w_size, j:j+w_size] += (w[:,None,:,:,:] * z[:, :, i, j].transpose(1,0)[:,:,None,None,None]).sum(axis=0) # numpy广播机制牛逼！\n",
    "    # print('dx: ', time() - time1)\n",
    "    \n",
    "    return x\n",
    "\n",
    "# 最大池化导数, stride = p_size, padding = 0\n",
    "def dmax_pooling(z: np.ndarray, p_size: int, argmax: np.ndarray):\n",
    "    \"\"\"\n",
    "        z: batch_size * channels * w * h\n",
    "    \"\"\"    \n",
    "    batch_size, c, z_size, _ = z.shape\n",
    "    x_size = z_size * p_size\n",
    "    dx = np.zeros([batch_size*c*z_size*z_size, p_size*p_size])\n",
    "    dx[np.arange(argmax.size), argmax] = z.flatten()\n",
    "    dx = dx.reshape([batch_size,c,z_size,z_size,p_size,p_size], order='C').transpose(0,1,4,5,2,3)\n",
    "    dxx = np.zeros([batch_size, c, x_size, x_size])\n",
    "    \n",
    "    for i in range(p_size):\n",
    "        for j in range(p_size):\n",
    "            dxx[:,:,i::p_size,j::p_size] = dx[:, :, i, j,:,:]\n",
    "    return dxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 类化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, input_size, output_size, alpha, batch_size):\n",
    "        self.alpha = alpha\n",
    "        self.batch_size = batch_size\n",
    "        self.w = 0.1*np.random.randn(output_size, input_size)\n",
    "        self.x = None\n",
    "\n",
    "    def __call__(self, x: np.ndarray):\n",
    "        self.x = x\n",
    "        return linear(x, self.w)\n",
    "    \n",
    "    def backward(self, output):\n",
    "        dl =  dlinear(self.x, self.w).T @ output\n",
    "        self.w = self.w - self.alpha * ((output @ self.x.T) / self.batch_size)\n",
    "        return dl\n",
    "\n",
    "\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "\n",
    "    def __call__(self, x: np.ndarray):\n",
    "        self.x = x\n",
    "        return relu(x)\n",
    "    \n",
    "    def backward(self, output):\n",
    "        return drelu(self.x) * output\n",
    "\n",
    "\n",
    "class Cov2d:\n",
    "    def __init__(self, input_channel, output_channel, kernel_size, alpha):\n",
    "        self.alpha = alpha\n",
    "        self.w = 0.1*np.random.randn(output_channel, input_channel, kernel_size, kernel_size)\n",
    "        self.x = None\n",
    "\n",
    "    def __call__(self, x: np.ndarray):\n",
    "        self.x = x\n",
    "        return cov2d(x, self.w)\n",
    "    \n",
    "    def backward(self, output):\n",
    "        # dc = dxcov2d(self.w, output)\n",
    "        self.w = self.w - self.alpha * dwcov2d(self.x, output)\n",
    "        # return dc\n",
    "\n",
    "\n",
    "class MaxPooling:\n",
    "    def __init__(self, kernel_size: int):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.argmax = None\n",
    "\n",
    "    def __call__(self, x: np.ndarray):\n",
    "        max_z, self.argmax = max_pooling(x, self.kernel_size)\n",
    "        return max_z\n",
    "\n",
    "    def backward(self, output):\n",
    "        return dmax_pooling(output, self.kernel_size, self.argmax)\n",
    "\n",
    "\n",
    "class Flatten:\n",
    "    def __init__(self):\n",
    "        self.shape = None\n",
    "    \n",
    "    def __call__(self, x: np.ndarray):\n",
    "        self.shape = x.shape\n",
    "        return flatten(x, self.shape[0])\n",
    "\n",
    "    def backward(self, output):\n",
    "        return reflatten(output, self.shape)\n",
    "\n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    def __init__(self, batch_size):\n",
    "        self.a = None\n",
    "        self.y = None\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __call__(self, x: np.ndarray, y: np.ndarray):\n",
    "        self.y = y\n",
    "        self.a = softmax(x)\n",
    "        return cross_entropy(self.a, y, self.batch_size)\n",
    "\n",
    "    def backward(self):\n",
    "        return dcross_entropy(self.a, self.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全连接神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, alpha, batch_size):\n",
    "        self.networks = [\n",
    "            Linear(784, 512, alpha, batch_size),\n",
    "            ReLU(),\n",
    "            Linear(512, 256, alpha, batch_size),\n",
    "            ReLU(),\n",
    "            Linear(256, 64, alpha, batch_size),\n",
    "            ReLU(),\n",
    "            Linear(64, 32, alpha, batch_size),\n",
    "            ReLU(),\n",
    "            Linear(32, 10, alpha, batch_size),\n",
    "        ]\n",
    "        self.loss_f = CrossEntropyLoss(batch_size)\n",
    "\n",
    "    def train(self, x, y):\n",
    "        x = x / 255\n",
    "        for n in self.networks:\n",
    "            x = n(x)\n",
    "        return self.loss_f(x, y)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x / 255\n",
    "        for n in self.networks:\n",
    "            x = n(x)\n",
    "        return x.argmax(axis=0)\n",
    "\n",
    "    def backward(self):\n",
    "        delta = self.loss_f.backward()\n",
    "        for n in reversed(self.networks):\n",
    "            delta = n.backward(delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self, alpha, batch_size):\n",
    "        self.networks = [\n",
    "            Cov2d(1, 4, 3, alpha),\n",
    "            MaxPooling(2),\n",
    "            Flatten(),\n",
    "            Linear(676, 512, alpha, batch_size),\n",
    "            ReLU(),\n",
    "            Linear(512, 256, alpha, batch_size),\n",
    "            ReLU(),\n",
    "            Linear(256, 10, alpha, batch_size),\n",
    "        ]\n",
    "        self.loss_f = CrossEntropyLoss(batch_size)\n",
    "    \n",
    "    def train(self, x, y):\n",
    "        x = x / 255\n",
    "        for n in self.networks:\n",
    "            x = n(x)\n",
    "        return self.loss_f(x, y)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x / 255\n",
    "        for n in self.networks:\n",
    "            x = n(x)\n",
    "        return x.argmax(axis=0)\n",
    "        \n",
    "\n",
    "    def backward(self):\n",
    "        delta = self.loss_f.backward()\n",
    "        for n in reversed(self.networks):\n",
    "            delta = n.backward(delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.MNIST('./', True)\n",
    "data = dataset.get()\n",
    "X_train, Y_train, X_test, Y_test = data\n",
    "Y_train = npy.vstack([onehot(i) for i in Y_train]).T\n",
    "Y_train = np.array(Y_train)\n",
    "Y_test = npy.vstack([onehot(i) for i in Y_test]).T\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096\n",
    "m = MLP(alpha=0.1, batch_size=batch_size)\n",
    "c = CNN(alpha=0.01, batch_size=batch_size)  # 学习率再大就溢出了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = npy.vstack([i.flatten() for i in X_train]).T\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "samples = int(X_train.shape[1] / batch_size)\n",
    "epoches = 10\n",
    "loss_m = []\n",
    "\n",
    "with tqdm(total=epoches) as t:\n",
    "    for i in range(epoches):\n",
    "        los = 0\n",
    "        for j in range(samples):\n",
    "            los += m.train(X_train[:,j*batch_size: (j+1)*batch_size], Y_train[:,j*batch_size: (j+1)*batch_size])\n",
    "            m.backward()\n",
    "            \n",
    "        \n",
    "        loss_m.append(los/samples)\n",
    "        t.set_description('Epoch {}'.format(i), refresh=False)\n",
    "        t.set_postfix(loss=los/samples, refresh=False)\n",
    "        t.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = data\n",
    "Y_train = npy.vstack([onehot(i) for i in Y_train]).T\n",
    "Y_train = np.array(Y_train)\n",
    "Y_test = npy.vstack([onehot(i) for i in Y_test]).T\n",
    "Y_test = np.array(Y_test)\n",
    "X_train = X_train[:,None,:,:]\n",
    "X_test = X_test[:,None,:,:]\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "\n",
    "samples = len(X_train) // batch_size\n",
    "epoches = 10\n",
    "\n",
    "loss_c = []\n",
    "\n",
    "for i in c.networks:\n",
    "    if('alpha' in dir(i)):\n",
    "        if('batch_size' in dir(i)):\n",
    "            i.alpha = 0.0001\n",
    "        else:\n",
    "            i.alpha = 0.001\n",
    "\n",
    "\n",
    "with tqdm(total=epoches) as t:\n",
    "    for i in range(epoches):\n",
    "        los = 0\n",
    "        for j in range(samples):\n",
    "            los += c.train(X_train[j*batch_size:(j+1)*batch_size], Y_train[:,j*batch_size:(j+1)*batch_size])\n",
    "            c.backward()\n",
    "            \n",
    "        loss_c.append(los/samples)\n",
    "        \n",
    "        t.set_description('Epoch {}'.format(i), refresh=False)\n",
    "        t.set_postfix(loss=los/samples, refresh=False)  # , acc=accuracy(c, X_train, y, len(X_train))\n",
    "        t.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.subplot(121)\n",
    "fig1.plot(range(len(loss_m)), np.array(loss_m).get())\n",
    "fig2 = plt.subplot(122)\n",
    "fig2.plot(range(len(loss_c)), np.array(loss_c).get())\n",
    "plt.title('Loss with epoches')\n",
    "plt.xlabel('epoches')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig('./loss.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beatsleo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "16ea64f9ee948d927ad35fd9dd41586a042d593dc7bf73dbea6b47fb27e81f20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
